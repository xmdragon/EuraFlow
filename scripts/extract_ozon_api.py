#!/usr/bin/env python3
"""
OZON APIæ–‡æ¡£æå–å·¥å…·
ä»HTMLæ–‡æ¡£ä¸­æå–æ‰€æœ‰APIæ¥å£å¹¶è½¬æ¢ä¸ºMarkdownæ ¼å¼
"""

import os
import re
import json
from pathlib import Path
from bs4 import BeautifulSoup
from typing import Dict, List, Optional, Tuple

# é…ç½®
HTML_FILE = "/mnt/e/pics/ozon.api.html"
OUTPUT_DIR = "docs/OzonAPI"


def extract_version_from_path(path: str) -> str:
    """ä»APIè·¯å¾„ä¸­æå–ç‰ˆæœ¬å·"""
    match = re.search(r'/v(\d+)/', path)
    if match:
        return f"v{match.group(1)}"
    return ""


def sanitize_filename(name: str) -> str:
    """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
    # æ›¿æ¢æˆ–åˆ é™¤éæ³•å­—ç¬¦
    name = name.replace('/', '_')
    name = name.replace('\\', '_')
    name = name.replace('?', '')
    name = name.replace('*', '')
    name = name.replace(':', '-')
    name = name.replace('<', '')
    name = name.replace('>', '')
    name = name.replace('|', '-')
    name = name.replace('"', '')
    # é™åˆ¶é•¿åº¦
    if len(name) > 100:
        name = name[:100]
    return name.strip()


def extract_text(element) -> str:
    """æå–å…ƒç´ çš„çº¯æ–‡æœ¬å†…å®¹"""
    if element is None:
        return ""
    return element.get_text(strip=True)


def extract_table_to_markdown(table) -> str:
    """å°†HTMLè¡¨æ ¼è½¬æ¢ä¸ºMarkdownè¡¨æ ¼"""
    if table is None:
        return ""

    rows = table.find_all('tr')
    if not rows:
        return ""

    markdown = []

    for i, row in enumerate(rows):
        cells = row.find_all(['td', 'th'])
        if not cells:
            continue

        # æå–å•å…ƒæ ¼å†…å®¹
        cell_contents = []
        for cell in cells:
            # æ¯ä¸ªtdå¯èƒ½åŒ…å«å¤šä¸ªå­å…ƒç´ ï¼Œéœ€è¦åˆ†åˆ«æå–
            # æŸ¥æ‰¾å‚æ•°å
            param_name_span = cell.find('span', class_='sc-ieebsP')
            if param_name_span:
                # è¿™æ˜¯å‚æ•°ååˆ—
                param_name = extract_text(param_name_span.find_next_sibling('span'))
            else:
                param_name = None

            # æŸ¥æ‰¾requiredæ ‡è®°
            required_div = cell.find('div', class_=re.compile('sc-jUotMc'))
            if required_div:
                required = extract_text(required_div).strip()
            else:
                required = None

            # å¦‚æœæœ‰å‚æ•°åå’Œrequiredï¼Œç»„åˆå®ƒä»¬
            if param_name and required:
                cell_contents.append(param_name)
                cell_contents.append(required)
            elif param_name:
                cell_contents.append(param_name)
            else:
                # æ™®é€šå•å…ƒæ ¼ï¼ŒæŸ¥æ‰¾ç±»å‹å’Œæè¿°
                type_span = cell.find('span', class_=re.compile('sc-kHOZQx.*sc-dtMiey|sc-hOGjNT'))
                description_div = cell.find('div', class_=re.compile('sc-efQUeY.*sc-hUpaWb'))

                if type_span or description_div:
                    # ç±»å‹
                    if type_span:
                        cell_type = extract_text(type_span)
                        cell_contents.append(cell_type)

                    # æè¿°
                    if description_div:
                        desc = extract_text(description_div)
                        if desc:
                            cell_contents.append(desc)
                else:
                    # ä½œä¸ºæ™®é€šå•å…ƒæ ¼å¤„ç†
                    text = extract_text(cell)
                    if text:
                        # è½¬ä¹‰Markdownç‰¹æ®Šå­—ç¬¦
                        text = text.replace('|', '\\|')
                        cell_contents.append(text)

        # å¦‚æœæ²¡æœ‰æå–åˆ°å†…å®¹ï¼Œè·³è¿‡è¿™ä¸€è¡Œ
        if not cell_contents:
            continue

        # æ„å»ºMarkdownè¡Œ
        markdown.append('| ' + ' | '.join(cell_contents) + ' |')

        # ç¬¬ä¸€è¡Œåæ·»åŠ åˆ†éš”ç¬¦
        if i == 0:
            markdown.append('|' + '|'.join(['---' for _ in cell_contents]) + '|')

    return '\n'.join(markdown)


def extract_json_from_code(element) -> str:
    """ä»ä»£ç å…ƒç´ ä¸­æå–JSON"""
    if element is None:
        return ""

    # æŸ¥æ‰¾JSONä»£ç å—
    code = element.find('code')
    if code:
        # æå–æ–‡æœ¬å¹¶æ¸…ç†
        text = code.get_text()
        # å°è¯•æ ¼å¼åŒ–JSON
        try:
            # ç§»é™¤å¯èƒ½çš„HTMLå®ä½“
            text = text.strip()
            # å¦‚æœæ˜¯æœ‰æ•ˆJSON,æ ¼å¼åŒ–å®ƒ
            parsed = json.loads(text)
            return json.dumps(parsed, indent=2, ensure_ascii=False)
        except:
            # å¦‚æœä¸æ˜¯æœ‰æ•ˆJSON,è¿”å›åŸå§‹æ–‡æœ¬
            return text

    return extract_text(element)


def extract_api_info(soup: BeautifulSoup, operation_id: str) -> Optional[Dict]:
    """æå–å•ä¸ªAPIçš„è¯¦ç»†ä¿¡æ¯"""

    # æŸ¥æ‰¾APIè¯¦æƒ…div
    detail_div = soup.find('div', {'id': operation_id})
    if not detail_div:
        print(f"  âš ï¸  æœªæ‰¾åˆ°è¯¦æƒ…: {operation_id}")
        return None

    api_info = {
        'operation_id': operation_id,
        'title': '',
        'method': '',
        'path': '',
        'description': '',
        'header_params': '',
        'request_body': '',
        'request_example': '',
        'responses': []
    }

    # æå–æ ‡é¢˜
    h2 = detail_div.find('h2')
    if h2:
        api_info['title'] = extract_text(h2)

    # æå–HTTPæ–¹æ³•å’Œè·¯å¾„
    http_verb = detail_div.find('span', class_=re.compile('http-verb'))
    if http_verb:
        api_info['method'] = extract_text(http_verb).upper()

    path_div = detail_div.find('div', class_=re.compile('sc-gIBoTZ|gNSSYl'))
    if path_div:
        api_info['path'] = extract_text(path_div)

    # æå–æè¿°
    desc_p = detail_div.find('p')
    if desc_p:
        api_info['description'] = extract_text(desc_p)

    # æå–Headerå‚æ•°
    header_section = detail_div.find('h5', string=re.compile('header Parameters'))
    if header_section:
        table = header_section.find_next('table')
        if table:
            api_info['header_params'] = extract_table_to_markdown(table)

    # æå–è¯·æ±‚ä½“
    request_body_section = detail_div.find('h5', string=re.compile('Request Body schema'))
    if request_body_section:
        # æŸ¥æ‰¾è¯·æ±‚ä½“è¡¨æ ¼
        table = request_body_section.find_next('table')
        if table:
            api_info['request_body'] = extract_table_to_markdown(table)

    # æå–è¯·æ±‚ç¤ºä¾‹
    request_example_h3 = detail_div.find('h3', string=re.compile('è¯·æ±‚èŒƒä¾‹'))
    if request_example_h3:
        example_div = request_example_h3.find_next('div', class_=re.compile('redoc-json'))
        if example_div:
            api_info['request_example'] = extract_json_from_code(example_div)

    # æå–å“åº”
    response_example_h3 = detail_div.find('h3', string=re.compile('å›å¤èŒƒä¾‹'))
    if response_example_h3:
        # æŸ¥æ‰¾æ‰€æœ‰å“åº”æ ‡ç­¾é¡µ
        tab_panels = response_example_h3.find_next_siblings('div', class_=re.compile('react-tabs__tab-panel'))
        for panel in tab_panels:
            response_div = panel.find('div', class_=re.compile('redoc-json'))
            if response_div:
                response_code = extract_json_from_code(response_div)
                # å°è¯•ä»æ ‡ç­¾ä¸­è·å–çŠ¶æ€ç 
                tab_list = response_example_h3.find_previous('ul', class_=re.compile('react-tabs__tab-list'))
                status_code = "200"
                if tab_list:
                    tabs = tab_list.find_all('li', class_=re.compile('tab-'))
                    if tabs:
                        status_code = extract_text(tabs[0])

                api_info['responses'].append({
                    'status': status_code,
                    'example': response_code
                })
                break

    # æå–å“åº”ç»“æ„
    response_section = detail_div.find('h5', string=re.compile('Response Schema'))
    if response_section:
        table = response_section.find_next('table')
        if table:
            api_info['response_schema'] = extract_table_to_markdown(table)
        else:
            api_info['response_schema'] = ''

    return api_info


def format_markdown(api_info: Dict) -> str:
    """å°†APIä¿¡æ¯æ ¼å¼åŒ–ä¸ºMarkdown"""

    md = []

    # æ ‡é¢˜
    md.append(f"# {api_info['title']}\n")

    # æ¥å£ä¿¡æ¯
    md.append("## æ¥å£ä¿¡æ¯\n")
    md.append(f"- **HTTPæ–¹æ³•**: `{api_info['method']}`")
    md.append(f"- **APIè·¯å¾„**: `{api_info['path']}`")
    md.append(f"- **æ“ä½œID**: `{api_info['operation_id']}`\n")

    # æè¿°
    if api_info['description']:
        md.append("## æè¿°\n")
        md.append(f"{api_info['description']}\n")

    # Headerå‚æ•°
    if api_info['header_params']:
        md.append("## è¯·æ±‚å‚æ•°\n")
        md.append("### Headerå‚æ•°\n")
        md.append(api_info['header_params'])
        md.append("")

    # è¯·æ±‚ä½“
    if api_info.get('request_body'):
        md.append("### è¯·æ±‚ä½“ç»“æ„\n")
        md.append(api_info['request_body'])
        md.append("")

    # è¯·æ±‚ç¤ºä¾‹
    if api_info['request_example']:
        md.append("## è¯·æ±‚ç¤ºä¾‹\n")
        md.append("```json")
        md.append(api_info['request_example'])
        md.append("```\n")

    # å“åº”
    md.append("## å“åº”\n")

    # å“åº”ç»“æ„
    if api_info.get('response_schema'):
        md.append("### å“åº”ç»“æ„\n")
        md.append(api_info['response_schema'])
        md.append("")

    # å“åº”ç¤ºä¾‹
    if api_info['responses']:
        for response in api_info['responses']:
            md.append(f"### {response['status']} å“åº”ç¤ºä¾‹\n")
            md.append("```json")
            md.append(response['example'])
            md.append("```\n")

    return '\n'.join(md)


def extract_all_apis():
    """æå–æ‰€æœ‰APIæ–‡æ¡£"""

    print(f"ğŸ“– è¯»å–HTMLæ–‡ä»¶: {HTML_FILE}")

    # è¯»å–HTMLæ–‡ä»¶
    with open(HTML_FILE, 'r', encoding='utf-8') as f:
        html_content = f.read()

    print(f"âœ… HTMLæ–‡ä»¶å¤§å°: {len(html_content) / 1024 / 1024:.2f} MB")

    # è§£æHTML
    print("ğŸ” è§£æHTMLæ–‡æ¡£...")
    soup = BeautifulSoup(html_content, 'lxml')

    # æŸ¥æ‰¾æ‰€æœ‰operationç±»å‹çš„API
    print("ğŸ” æŸ¥æ‰¾æ‰€æœ‰APIæ“ä½œ...")
    operation_items = soup.find_all('li', {'data-item-id': re.compile(r'^operation/')})

    print(f"âœ… æ‰¾åˆ° {len(operation_items)} ä¸ªAPIæ“ä½œ\n")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(OUTPUT_DIR)
    output_path.mkdir(parents=True, exist_ok=True)

    # æå–æ¯ä¸ªAPI
    api_list = []
    success_count = 0
    filename_counter = {}  # è·Ÿè¸ªæ–‡ä»¶åä½¿ç”¨æƒ…å†µ

    for i, item in enumerate(operation_items, 1):
        operation_id = item.get('data-item-id')

        # æå–APIåç§°
        label = item.find('label', {'type': 'operation'})
        if not label:
            continue

        # æå–ä¸­æ–‡åç§°
        name_div = label.find('div', class_=re.compile('sc-fXEqXD'))
        api_name = extract_text(name_div) if name_div else operation_id

        # æå–HTTPæ–¹æ³•
        method_span = label.find('span', class_=re.compile('operation-type'))
        method = extract_text(method_span).upper() if method_span else 'POST'

        # æå–è·¯å¾„
        path_div = label.find('div', class_=re.compile('sc-FNZbm'))
        path = extract_text(path_div) if path_div else ''

        print(f"[{i}/{len(operation_items)}] {api_name}")
        print(f"    {method} {path}")

        # æå–è¯¦ç»†ä¿¡æ¯
        api_info = extract_api_info(soup, operation_id)

        if api_info:
            # è¡¥å……åŸºæœ¬ä¿¡æ¯
            api_info['title'] = api_name
            api_info['method'] = method
            api_info['path'] = path

            # ç”ŸæˆMarkdown
            markdown = format_markdown(api_info)

            # ç”Ÿæˆæ–‡ä»¶åï¼ˆå¸¦ç‰ˆæœ¬å·å»é‡ï¼‰
            base_filename = sanitize_filename(api_name)

            # æå–ç‰ˆæœ¬å·
            version = extract_version_from_path(path)

            # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦å·²å­˜åœ¨
            if base_filename in filename_counter:
                # å¦‚æœæœ‰ç‰ˆæœ¬å·ï¼Œæ·»åŠ åˆ°æ–‡ä»¶å
                if version:
                    filename = f"{base_filename}_{version}.md"
                    print(f"    âš ï¸  æ–‡ä»¶åé‡å¤ï¼Œæ·»åŠ ç‰ˆæœ¬å·: {version}")
                else:
                    # æ²¡æœ‰ç‰ˆæœ¬å·ï¼Œä½¿ç”¨è®¡æ•°å™¨
                    count = filename_counter[base_filename]
                    filename_counter[base_filename] += 1
                    filename = f"{base_filename}_{count}.md"
                    print(f"    âš ï¸  æ–‡ä»¶åé‡å¤ï¼Œæ·»åŠ åºå·: {count}")
            else:
                filename = base_filename + '.md'
                filename_counter[base_filename] = 1

            filepath = output_path / filename

            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(markdown)

            print(f"    âœ… å·²ä¿å­˜: {filename}\n")

            api_list.append({
                'name': api_name,
                'method': method,
                'path': path,
                'filename': filename,
                'operation_id': operation_id,
                'version': version
            })

            success_count += 1
        else:
            print(f"    âŒ æå–å¤±è´¥\n")

    print(f"\n{'='*60}")
    print(f"âœ… æå–å®Œæˆï¼")
    print(f"   æˆåŠŸ: {success_count}/{len(operation_items)}")
    print(f"   è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"{'='*60}\n")

    return api_list


def generate_index(api_list: List[Dict]):
    """ç”Ÿæˆç´¢å¼•æ–‡ä»¶"""

    print("ğŸ“ ç”Ÿæˆç´¢å¼•æ–‡ä»¶...")

    md = []
    md.append("# OZON Seller API æ–‡æ¡£\n")
    md.append(f"å…± {len(api_list)} ä¸ªAPIæ¥å£\n")
    md.append("## APIåˆ—è¡¨\n")

    # æŒ‰åŠŸèƒ½åˆ†ç»„
    grouped = {}
    for api in api_list:
        # æ ¹æ®è·¯å¾„å‰ç¼€åˆ†ç»„
        path_parts = api['path'].split('/')
        if len(path_parts) >= 2:
            group = path_parts[1]  # ä¾‹å¦‚ v1, v2, v3
            if len(path_parts) >= 3:
                group = path_parts[2]  # ä¾‹å¦‚ product, order
        else:
            group = "å…¶ä»–"

        if group not in grouped:
            grouped[group] = []
        grouped[group].append(api)

    # è¾“å‡ºåˆ†ç»„
    for group in sorted(grouped.keys()):
        md.append(f"### {group}\n")
        for api in sorted(grouped[group], key=lambda x: x['name']):
            md.append(f"- [{api['name']}](./{api['filename']}) - `{api['method']} {api['path']}`")
        md.append("")

    # ä¿å­˜ç´¢å¼•
    index_path = Path(OUTPUT_DIR) / 'README.md'
    with open(index_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(md))

    print(f"âœ… ç´¢å¼•æ–‡ä»¶å·²ç”Ÿæˆ: {index_path}\n")


if __name__ == '__main__':
    print("\n" + "="*60)
    print("ğŸš€ OZON API æ–‡æ¡£æå–å·¥å…·")
    print("="*60 + "\n")

    api_list = extract_all_apis()

    if api_list:
        generate_index(api_list)

    print("ğŸ‰ å…¨éƒ¨å®Œæˆï¼\n")
