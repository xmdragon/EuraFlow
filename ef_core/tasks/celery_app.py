"""
Celery åº”ç”¨é…ç½®
"""
from celery import Celery, signals
from celery.schedules import crontab
from ef_core.config import get_settings
from ef_core.utils.logger import get_logger

logger = get_logger(__name__)

# èŽ·å–é…ç½®
settings = get_settings()

# åˆ›å»º Celery åº”ç”¨
celery_app = Celery(
    "euraflow",
    broker=settings.celery_broker_url,
    backend=settings.celery_result_backend,
    include=[
        "ef_core.tasks.core_tasks",
        "plugins.ef.channels.ozon.tasks",  # Ozonæ’ä»¶ä»»åŠ¡
    ]
)

# Celery é…ç½®
celery_app.conf.update(
    # ä»»åŠ¡åºåˆ—åŒ–
    task_serializer=settings.celery_task_serializer,
    result_serializer=settings.celery_result_serializer,
    accept_content=settings.celery_accept_content,
    
    # æ—¶åŒºè®¾ç½®
    timezone=settings.celery_timezone,
    enable_utc=True,
    
    # ä»»åŠ¡è·¯ç”±
    task_routes={
        "ef.core.*": {"queue": "ef_core"},
        "ef.*.pull_orders": {"queue": "ef_pull"},
        "ef.*.push_*": {"queue": "ef_push"},
    },
    
    # ä»»åŠ¡ç»“æžœé…ç½®
    result_expires=3600,  # 1å°æ—¶åŽè¿‡æœŸ
    task_ignore_result=False,
    
    # Worker é…ç½®
    worker_prefetch_multiplier=1,  # å…¬å¹³è°ƒåº¦
    task_acks_late=True,  # ä»»åŠ¡å®ŒæˆåŽç¡®è®¤
    worker_max_tasks_per_child=1000,  # æ¯ä¸ªworkerå¤„ç†1000ä¸ªä»»åŠ¡åŽé‡å¯
    
    # é‡è¯•é…ç½®
    task_default_retry_delay=60,  # 60ç§’åŽé‡è¯•
    task_max_retries=5,  # æœ€å¤šé‡è¯•5æ¬¡
    
    # é€ŸçŽ‡é™åˆ¶
    task_annotations={
        "*": {"rate_limit": "100/m"},  # å…¨å±€é™åˆ¶
    },
    
    # ç›‘æŽ§é…ç½®
    worker_send_task_events=True,
    task_send_sent_event=True,
    
    # ä»»åŠ¡åŽ‹ç¼©
    task_compression="gzip",
    result_compression="gzip",
)

# å®šæœŸä»»åŠ¡é…ç½®ï¼ˆBeat Scheduleï¼‰
celery_app.conf.beat_schedule = {
    # ç³»ç»Ÿå¥åº·æ£€æŸ¥
    "system-health-check": {
        "task": "ef.core.health_check",
        "schedule": crontab(minute="*/5"),  # æ¯5åˆ†é’Ÿ
        "options": {"queue": "ef_core"}
    },

    # æ¸…ç†è¿‡æœŸä»»åŠ¡ç»“æžœ
    "cleanup-expired-results": {
        "task": "ef.core.cleanup_results",
        "schedule": crontab(hour="2", minute="0"),  # æ¯å¤©å‡Œæ™¨2ç‚¹
        "options": {"queue": "ef_core"}
    },

    # æ³¨æ„ï¼šOZON ç±»ç›®åŒæ­¥å’Œç‰¹å¾åŒæ­¥ä»»åŠ¡å·²ç§»é™¤ï¼ˆæœªå®žçŽ°å¯¹åº”çš„ä»»åŠ¡å‡½æ•°ï¼‰
    # å¦‚éœ€å¯ç”¨ï¼Œéœ€åœ¨ OZON æ’ä»¶ä¸­æ³¨å†Œå¯¹åº”çš„ Celery ä»»åŠ¡

    # æ’ä»¶ä»»åŠ¡å°†é€šè¿‡ TaskRegistry åŠ¨æ€æ³¨å†Œ
}


@celery_app.task(bind=True, name="ef.core.health_check")
def health_check(self):
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥ä»»åŠ¡"""
    try:
        logger.info("Performing system health check")
        
        # TODO: æ£€æŸ¥æ•°æ®åº“è¿žæŽ¥
        # TODO: æ£€æŸ¥ Redis è¿žæŽ¥
        # TODO: æ£€æŸ¥å¤–éƒ¨ API è¿žæŽ¥
        
        logger.info("System health check completed", result="healthy")
        return {"status": "healthy", "timestamp": "2025-01-01T00:00:00Z"}
    
    except Exception as e:
        logger.error("System health check failed", exc_info=True)
        raise self.retry(countdown=60, max_retries=3)


@celery_app.task(bind=True, name="ef.core.cleanup_results")
def cleanup_results(self):
    """æ¸…ç†è¿‡æœŸçš„ä»»åŠ¡ç»“æžœ"""
    try:
        logger.info("Cleaning up expired task results")
        
        # TODO: æ¸…ç† Redis ä¸­è¿‡æœŸçš„ä»»åŠ¡ç»“æžœ
        
        logger.info("Task results cleanup completed")
        return {"cleaned": 0, "timestamp": "2025-01-01T00:00:00Z"}
    
    except Exception as e:
        logger.error("Task results cleanup failed", exc_info=True)
        return {"error": str(e)}


# Celery ä¿¡å·å¤„ç†å™¨
@signals.task_prerun.connect
def task_prerun_handler(sender=None, task_id=None, task=None, args=None, kwargs=None, **kwds):
    """ä»»åŠ¡å¼€å§‹å‰çš„å¤„ç†"""
    logger.info(f"Task starting: {task.name}", task_id=task_id)


@signals.task_postrun.connect
def task_postrun_handler(sender=None, task_id=None, task=None, args=None, kwargs=None, retval=None, state=None, **kwds):
    """ä»»åŠ¡å®ŒæˆåŽçš„å¤„ç†"""
    logger.info(f"Task completed: {task.name}", task_id=task_id, state=state)


@signals.task_failure.connect
def task_failure_handler(sender=None, task_id=None, exception=None, traceback=None, einfo=None, **kwds):
    """ä»»åŠ¡å¤±è´¥çš„å¤„ç†"""
    logger.error(f"Task failed: {sender.name}", task_id=task_id, exception=str(exception))


@signals.task_retry.connect
def task_retry_handler(sender=None, task_id=None, reason=None, einfo=None, **kwds):
    """ä»»åŠ¡é‡è¯•çš„å¤„ç†"""
    logger.warning(f"Task retrying: {sender.name}", task_id=task_id, reason=str(reason))


# ====================================================================================
# å…³é”®ï¼šåœ¨æ¨¡å—åŠ è½½æ—¶åˆå§‹åŒ–æ’ä»¶ï¼ˆç¡®ä¿ Celery Beat å’Œ Worker éƒ½èƒ½åŠ è½½æ’ä»¶ä»»åŠ¡ï¼‰
# ====================================================================================
def _initialize_plugins_for_celery():
    """
    Celery æ¨¡å—åŠ è½½æ—¶åˆå§‹åŒ–æ’ä»¶å¹¶æ³¨å†Œå®šæ—¶ä»»åŠ¡

    è¿™æ˜¯å…³é”®ï¼Celery Beat ä½œä¸ºç‹¬ç«‹è¿›ç¨‹å¯åŠ¨ï¼Œä¸ä¼šè§¦å‘ FastAPI çš„ startup äº‹ä»¶ï¼Œ
    æ‰€ä»¥å¿…é¡»åœ¨æ¨¡å—åŠ è½½æ—¶æ˜¾å¼åˆå§‹åŒ–æ’ä»¶ï¼Œå¦åˆ™æ’ä»¶æ³¨å†Œçš„å®šæ—¶ä»»åŠ¡ä¸ä¼šè¢«è°ƒåº¦ã€‚

    æ”¾åœ¨æ¨¡å—çº§åˆ«æ‰§è¡Œï¼Œç¡®ä¿æ— è®ºæ˜¯ Beat è¿˜æ˜¯ Workerï¼Œåªè¦å¯¼å…¥äº† celery_app å°±ä¼šåˆå§‹åŒ–ã€‚
    """
    try:
        import asyncio
        from ef_core.plugin_host import get_plugin_host
        from ef_core.tasks.registry import TaskRegistry
        from ef_core.event_bus import EventBus

        logger.info("ðŸ”§ Initializing plugins for Celery...")

        # åœ¨åŒä¸€ä¸ªäº‹ä»¶å¾ªçŽ¯ä¸­å®Œæˆæ‰€æœ‰å¼‚æ­¥åˆå§‹åŒ–
        async def async_init():
            # åˆ›å»ºä»»åŠ¡æ³¨å†Œè¡¨å’Œäº‹ä»¶æ€»çº¿
            task_registry = TaskRegistry()
            event_bus = EventBus()

            try:
                # åˆå§‹åŒ–äº‹ä»¶æ€»çº¿
                await event_bus.initialize()

                # èŽ·å–æ’ä»¶å®¿ä¸»å¹¶æ³¨å…¥ä¾èµ–
                plugin_host = get_plugin_host()
                plugin_host.task_registry = task_registry
                plugin_host.event_bus = event_bus

                # åˆå§‹åŒ–æ‰€æœ‰æ’ä»¶ï¼ˆä¼šè°ƒç”¨æ¯ä¸ªæ’ä»¶çš„ setup() å¹¶æ³¨å†Œå®šæ—¶ä»»åŠ¡ï¼‰
                await plugin_host.initialize()

                return task_registry
            finally:
                # å…³é”®ä¿®å¤ï¼šåœ¨äº‹ä»¶å¾ªçŽ¯ç»“æŸå‰æ­£ç¡®å…³é—­ EventBusï¼Œé¿å… "Event loop is closed" é”™è¯¯
                await event_bus.shutdown()

        # åœ¨å•ä¸ªäº‹ä»¶å¾ªçŽ¯ä¸­æ‰§è¡Œæ‰€æœ‰å¼‚æ­¥æ“ä½œ
        task_registry = asyncio.run(async_init())

        logger.info(f"âœ… Celery plugin initialization completed, registered {len(task_registry.registered_tasks)} tasks")

        # è¾“å‡ºæ³¨å†Œçš„ä»»åŠ¡åˆ—è¡¨ï¼ˆä¾¿äºŽè°ƒè¯•ï¼‰
        for task_name in task_registry.registered_tasks.keys():
            logger.info(f"  ðŸ“‹ Registered task: {task_name}")

    except Exception as e:
        logger.error(f"âŒ Failed to initialize plugins for Celery: {e}", exc_info=True)
        import traceback
        traceback.print_exc()

# ç«‹å³æ‰§è¡Œæ’ä»¶åˆå§‹åŒ–
_initialize_plugins_for_celery()


@signals.worker_ready.connect
def cleanup_stale_tasks_on_startup(**kwargs):
    """
    Worker å¯åŠ¨æ—¶æ¸…ç†æ‰€æœ‰åƒµæ­»çš„ä»»åŠ¡è¿›åº¦è®°å½•

    å½“ Celery worker é‡å¯æ—¶ï¼ŒRedis ä¸­å¯èƒ½è¿˜ä¿ç•™ç€æ—§çš„ä»»åŠ¡è¿›åº¦çŠ¶æ€ã€‚
    è¿™äº›çŠ¶æ€ä¼šå¯¼è‡´å‰ç«¯è®¤ä¸ºä»»åŠ¡è¿˜åœ¨è¿è¡Œï¼Œä½†å®žé™…ä¸Š worker å·²ç»é‡å¯ï¼Œä»»åŠ¡å·²ç»ä¸¢å¤±ã€‚
    """
    try:
        import redis
        import json
        from celery.result import AsyncResult

        redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
        cleaned_count = 0
        zombie_count = 0

        for key in redis_client.keys("celery-task-progress:*"):
            try:
                data = redis_client.get(key)
                if not data:
                    continue

                progress = json.loads(data)
                task_id = key.replace("celery-task-progress:", "")

                # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
                if progress.get('status') in ['starting', 'syncing']:
                    # æ£€æŸ¥ Celery ä¸­ä»»åŠ¡æ˜¯å¦çœŸçš„åœ¨è¿è¡Œ
                    result = AsyncResult(task_id, app=celery_app)

                    # PENDING è¡¨ç¤ºä»»åŠ¡ä¸å­˜åœ¨æˆ–æœªå¼€å§‹ï¼ŒFAILURE/SUCCESS/REVOKED è¡¨ç¤ºä»»åŠ¡å·²ç»“æŸ
                    if result.state in ['PENDING', 'FAILURE', 'SUCCESS', 'REVOKED']:
                        redis_client.delete(key)
                        zombie_count += 1
                        logger.warning(f"Cleaned zombie task {task_id} with Celery state {result.state}")
                    else:
                        logger.info(f"Task {task_id} is still running with state {result.state}")
                elif progress.get('status') in ['completed', 'failed']:
                    # å·²å®Œæˆçš„ä»»åŠ¡ï¼Œæ¸…ç†æŽ‰
                    redis_client.delete(key)
                    cleaned_count += 1

            except Exception as e:
                logger.error(f"Error cleaning task progress {key}: {e}", exc_info=True)

        if zombie_count > 0 or cleaned_count > 0:
            logger.info(
                f"Startup cleanup completed: {zombie_count} zombie tasks, {cleaned_count} completed tasks removed"
            )
    except Exception as e:
        logger.error(f"Failed to cleanup stale tasks on startup: {e}", exc_info=True)