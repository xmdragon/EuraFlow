"""
Celery åº”ç”¨é…ç½®
"""
from celery import Celery, signals
from celery.schedules import crontab
from ef_core.config import get_settings
from ef_core.utils.logger import get_logger

logger = get_logger(__name__)

# è·å–é…ç½®
settings = get_settings()

# åˆ›å»º Celery åº”ç”¨
celery_app = Celery(
    "euraflow",
    broker=settings.celery_broker_url,
    backend=settings.celery_result_backend,
    include=[
        "ef_core.tasks.core_tasks",
        "plugins.ef.channels.ozon.tasks",  # Ozonæ’ä»¶ä»»åŠ¡
    ]
)

# Celery é…ç½®
celery_app.conf.update(
    # ä»»åŠ¡åºåˆ—åŒ–
    task_serializer=settings.celery_task_serializer,
    result_serializer=settings.celery_result_serializer,
    accept_content=settings.celery_accept_content,
    
    # æ—¶åŒºè®¾ç½®
    timezone=settings.celery_timezone,
    enable_utc=True,
    
    # ä»»åŠ¡è·¯ç”±
    task_routes={
        "ef.core.*": {"queue": "ef_core"},
        "ef.*.pull_orders": {"queue": "ef_pull"},
        "ef.*.push_*": {"queue": "ef_push"},
    },
    
    # ä»»åŠ¡ç»“æœé…ç½®
    result_expires=3600,  # 1å°æ—¶åè¿‡æœŸ
    task_ignore_result=False,
    
    # Worker é…ç½®
    worker_prefetch_multiplier=1,  # å…¬å¹³è°ƒåº¦
    task_acks_late=True,  # ä»»åŠ¡å®Œæˆåç¡®è®¤
    worker_max_tasks_per_child=1000,  # æ¯ä¸ªworkerå¤„ç†1000ä¸ªä»»åŠ¡åé‡å¯
    
    # é‡è¯•é…ç½®
    task_default_retry_delay=60,  # 60ç§’åé‡è¯•
    task_max_retries=5,  # æœ€å¤šé‡è¯•5æ¬¡
    
    # é€Ÿç‡é™åˆ¶
    task_annotations={
        "*": {"rate_limit": "100/m"},  # å…¨å±€é™åˆ¶
    },
    
    # ç›‘æ§é…ç½®
    worker_send_task_events=True,
    task_send_sent_event=True,
    
    # ä»»åŠ¡å‹ç¼©
    task_compression="gzip",
    result_compression="gzip",
)

# å®šæœŸä»»åŠ¡é…ç½®ï¼ˆBeat Scheduleï¼‰
celery_app.conf.beat_schedule = {
    # ç³»ç»Ÿå¥åº·æ£€æŸ¥
    "system-health-check": {
        "task": "ef.core.health_check",
        "schedule": crontab(minute="*/5"),  # æ¯5åˆ†é’Ÿ
        "options": {"queue": "ef_core"}
    },

    # æ¸…ç†è¿‡æœŸä»»åŠ¡ç»“æœ
    "cleanup-expired-results": {
        "task": "ef.core.cleanup_results",
        "schedule": crontab(hour="2", minute="0"),  # æ¯å¤©å‡Œæ™¨2ç‚¹
        "options": {"queue": "ef_core"}
    },

    # æ³¨æ„ï¼šOZON ç±»ç›®åŒæ­¥å’Œç‰¹å¾åŒæ­¥ä»»åŠ¡å·²ç§»é™¤ï¼ˆæœªå®ç°å¯¹åº”çš„ä»»åŠ¡å‡½æ•°ï¼‰
    # å¦‚éœ€å¯ç”¨ï¼Œéœ€åœ¨ OZON æ’ä»¶ä¸­æ³¨å†Œå¯¹åº”çš„ Celery ä»»åŠ¡

    # æ’ä»¶ä»»åŠ¡å°†é€šè¿‡ TaskRegistry åŠ¨æ€æ³¨å†Œ
}


@celery_app.task(bind=True, name="ef.core.health_check")
def health_check(self):
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥ä»»åŠ¡"""
    try:
        logger.info("Performing system health check")
        
        # TODO: æ£€æŸ¥æ•°æ®åº“è¿æ¥
        # TODO: æ£€æŸ¥ Redis è¿æ¥
        # TODO: æ£€æŸ¥å¤–éƒ¨ API è¿æ¥
        
        logger.info("System health check completed", result="healthy")
        return {"status": "healthy", "timestamp": "2025-01-01T00:00:00Z"}
    
    except Exception as e:
        logger.error("System health check failed", exc_info=True)
        raise self.retry(countdown=60, max_retries=3)


@celery_app.task(bind=True, name="ef.core.cleanup_results")
def cleanup_results(self, older_than_days=1):
    """
    æ¸…ç†è¿‡æœŸçš„ä»»åŠ¡ç»“æœ

    Args:
        older_than_days: æ¸…ç†å¤šå°‘å¤©å‰çš„ç»“æœï¼ˆé»˜è®¤1å¤©ï¼‰

    Returns:
        æ¸…ç†ç»Ÿè®¡ä¿¡æ¯
    """
    import time
    from datetime import datetime, UTC

    try:
        logger.info(f"Starting cleanup of task results older than {older_than_days} days")

        # è·å– Redis è¿æ¥
        redis_client = celery_app.backend.client

        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "started_at": datetime.now(UTC).isoformat(),
            "scanned": 0,
            "cleaned": 0,
            "errors": 0,
            "older_than_days": older_than_days
        }

        # è®¡ç®—è¿‡æœŸæ—¶é—´æˆ³ï¼ˆç§’ï¼‰
        cutoff_timestamp = time.time() - (older_than_days * 24 * 3600)

        # æ‰«ææ‰€æœ‰ Celery ä»»åŠ¡ç»“æœé”®
        # Celery é»˜è®¤ä½¿ç”¨ "celery-task-meta-{task_id}" ä½œä¸ºé”®å
        cursor = 0
        batch_size = 1000

        while True:
            # ä½¿ç”¨ SCAN å‘½ä»¤æ‰¹é‡æ‰«æé”®ï¼ˆé¿å…é˜»å¡ Redisï¼‰
            cursor, keys = redis_client.scan(
                cursor=cursor,
                match="celery-task-meta-*",
                count=batch_size
            )

            stats["scanned"] += len(keys)

            # æ£€æŸ¥æ¯ä¸ªé”®
            for key in keys:
                try:
                    # è·å–é”®çš„ TTLï¼ˆå‰©ä½™ç”Ÿå­˜æ—¶é—´ï¼‰
                    ttl = redis_client.ttl(key)

                    # TTL = -1 è¡¨ç¤ºæ°¸ä¸è¿‡æœŸï¼ŒTTL = -2 è¡¨ç¤ºé”®ä¸å­˜åœ¨
                    if ttl == -1:
                        # å¯¹äºæ°¸ä¸è¿‡æœŸçš„é”®ï¼Œæ£€æŸ¥åˆ›å»ºæ—¶é—´
                        # å°è¯•è·å–ä»»åŠ¡ç»“æœçš„æ—¶é—´æˆ³
                        result = redis_client.get(key)
                        if result:
                            # Celery ç»“æœåŒ…å« date_done å­—æ®µ
                            import json
                            try:
                                result_data = json.loads(result)
                                date_done = result_data.get('date_done')

                                if date_done:
                                    # è§£ææ—¶é—´æˆ³
                                    from dateutil import parser
                                    done_time = parser.parse(date_done)
                                    done_timestamp = done_time.timestamp()

                                    # å¦‚æœè¶…è¿‡é˜ˆå€¼ï¼Œåˆ é™¤
                                    if done_timestamp < cutoff_timestamp:
                                        redis_client.delete(key)
                                        stats["cleaned"] += 1
                            except (json.JSONDecodeError, ValueError):
                                # æ— æ³•è§£æçš„ç»“æœï¼Œè·³è¿‡
                                pass

                    elif ttl == -2:
                        # é”®å·²ç»ä¸å­˜åœ¨ï¼Œè·³è¿‡
                        pass

                except Exception as e:
                    logger.warning(f"Error processing key {key}: {e}")
                    stats["errors"] += 1

            # å¦‚æœæ¸¸æ ‡å›åˆ°0ï¼Œè¡¨ç¤ºæ‰«æå®Œæˆ
            if cursor == 0:
                break

        # é¢å¤–æ¸…ç†ï¼šåˆ é™¤å·²è¿‡æœŸä½†æœªè¢«è‡ªåŠ¨æ¸…ç†çš„é”®
        # Celery æœ‰æ—¶ä¼šç•™ä¸‹ä¸€äº›è¿‡æœŸé”®
        cursor = 0
        while True:
            cursor, keys = redis_client.scan(
                cursor=cursor,
                match="celery-task-meta-*",
                count=batch_size
            )

            for key in keys:
                try:
                    ttl = redis_client.ttl(key)
                    # å¦‚æœ TTL <= 0ï¼ˆå·²è¿‡æœŸï¼‰ï¼Œç›´æ¥åˆ é™¤
                    if ttl == -2 or (ttl > 0 and ttl < 0):
                        redis_client.delete(key)
                        stats["cleaned"] += 1
                except Exception as e:
                    stats["errors"] += 1

            if cursor == 0:
                break

        # è®°å½•å®Œæˆæ—¶é—´
        stats["completed_at"] = datetime.now(UTC).isoformat()

        logger.info(
            f"Task results cleanup completed",
            scanned=stats["scanned"],
            cleaned=stats["cleaned"],
            errors=stats["errors"]
        )

        return stats

    except Exception as e:
        logger.error("Task results cleanup failed", exc_info=True)
        return {
            "error": str(e),
            "scanned": 0,
            "cleaned": 0,
            "errors": 1
        }


# Celery ä»»åŠ¡ååˆ° service_key çš„æ˜ å°„
TASK_TO_SERVICE_KEY_MAPPING = {
    "ef.system.database_backup": "database_backup",
    "ef.ozon.kuajing84.material_cost": "kuajing84_material_cost",
    "ef.ozon.finance.sync": "ozon_finance_sync",
    "ef.ozon.finance.transactions": "ozon_finance_transactions_daily",
    "ef.ozon.orders.pull": "ozon_sync_incremental",
    "ef.finance.rates.refresh": "exchange_rate_refresh",
    "ef.ozon.promotions.sync": "ozon_promotion_sync",
    # æ³¨æ„ï¼šä»¥ä¸‹ä»»åŠ¡æ²¡æœ‰å¯¹åº”çš„ sync_service è®°å½•ï¼Œä¸éœ€è¦ç»Ÿè®¡
    # ef.ozon.inventory.sync
    # ef.ozon.promotions.health_check  ï¼ˆå¥åº·æ£€æŸ¥ä¸éœ€è¦ç»Ÿè®¡ï¼‰
    # ef.ozon.category.sync
    # ef.ozon.attributes.sync
}

# ç¼“å­˜ SyncService æ¨¡å‹ç±»ï¼ˆé¿å…é‡å¤å¯¼å…¥å¯¼è‡´ SQLAlchemy è¡¨é‡å®šä¹‰é”™è¯¯ï¼‰
_sync_service_model = None


def _update_service_stats(task_name: str, success: bool, task_id: str, error_message: str = None):
    """
    æ›´æ–°åŒæ­¥æœåŠ¡ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨åŒæ­¥æ•°æ®åº“æ“ä½œï¼‰

    Args:
        task_name: Celery ä»»åŠ¡å
        success: æ˜¯å¦æˆåŠŸ
        task_id: ä»»åŠ¡ID
        error_message: é”™è¯¯ä¿¡æ¯ï¼ˆå¤±è´¥æ—¶ï¼‰
    """
    print(f"[DEBUG] _update_service_stats called: task_name={task_name}, success={success}", flush=True)

    # æ£€æŸ¥æ˜¯å¦æ˜¯éœ€è¦ç»Ÿè®¡çš„æœåŠ¡ä»»åŠ¡
    service_key = TASK_TO_SERVICE_KEY_MAPPING.get(task_name)
    if not service_key:
        # ä¸æ˜¯æ³¨å†Œçš„æœåŠ¡ä»»åŠ¡ï¼Œè·³è¿‡ç»Ÿè®¡
        print(f"[DEBUG] No service_key mapping for task {task_name}, skipping stats update", flush=True)
        return

    print(f"[DEBUG] Updating stats for service_key={service_key}", flush=True)

    try:
        from datetime import datetime, UTC
        from sqlalchemy import create_engine, select
        from sqlalchemy.orm import sessionmaker
        from ef_core.config import get_settings

        # ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹ç±»
        global _sync_service_model
        if _sync_service_model is None:
            from plugins.ef.system.sync_service.models.sync_service import SyncService
            _sync_service_model = SyncService

        SyncService = _sync_service_model

        # åˆ›å»ºåŒæ­¥æ•°æ®åº“å¼•æ“ï¼ˆé€‚ç”¨äº gevent ç¯å¢ƒï¼‰
        settings = get_settings()
        sync_db_url = settings.database_url.replace('+asyncpg', '')  # ç§»é™¤ asyncpgï¼Œä½¿ç”¨ psycopg2
        engine = create_engine(sync_db_url, pool_pre_ping=True, pool_recycle=3600)
        SessionLocal = sessionmaker(bind=engine)

        with SessionLocal() as db:
            # æŸ¥è¯¢æœåŠ¡è®°å½•
            stmt = select(SyncService).where(SyncService.service_key == service_key)
            service = db.execute(stmt).scalar_one_or_none()

            if not service:
                logger.warning(f"Service not found for task {task_name} (service_key: {service_key})")
                print(f"[DEBUG] âš ï¸  Service not found: {service_key}", flush=True)
                return

            # æ›´æ–°ç»Ÿè®¡å­—æ®µ
            service.run_count = (service.run_count or 0) + 1
            service.last_run_at = datetime.now(UTC)

            if success:
                service.success_count = (service.success_count or 0) + 1
                service.last_run_status = "success"
                service.last_run_message = "ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ"
            else:
                service.error_count = (service.error_count or 0) + 1
                service.last_run_status = "error"
                service.last_run_message = error_message or "ä»»åŠ¡æ‰§è¡Œå¤±è´¥"

            db.commit()

            logger.info(
                f"Updated stats for service {service_key}: "
                f"run_count={service.run_count}, "
                f"success_count={service.success_count}, "
                f"error_count={service.error_count}"
            )
            print(
                f"[DEBUG] âœ… Stats updated successfully: service_key={service_key}, "
                f"run_count={service.run_count}, success_count={service.success_count}, "
                f"error_count={service.error_count}",
                flush=True
            )

    except Exception as e:
        logger.error(f"Failed to update service stats for {task_name}: {e}", exc_info=True)
        print(f"[DEBUG] âŒ Failed to update stats: {e}", flush=True)


# Celery ä¿¡å·å¤„ç†å™¨ï¼ˆæ³¨æ„ï¼šå¿…é¡»åœ¨æ¨¡å—çº§åˆ«å®šä¹‰ï¼ŒWorker å¯åŠ¨æ—¶ä¼šè‡ªåŠ¨æ³¨å†Œï¼‰
@signals.task_prerun.connect
def task_prerun_handler(sender=None, task_id=None, task=None, args=None, kwargs=None, **kwds):
    """ä»»åŠ¡å¼€å§‹å‰çš„å¤„ç†"""
    print(f"[DEBUG] Task prerun: {task.name}, task_id={task_id}", flush=True)
    logger.info(f"Task starting: {task.name}", task_id=task_id)


@signals.task_postrun.connect
def task_postrun_handler(sender=None, task_id=None, task=None, args=None, kwargs=None, retval=None, state=None, **kwds):
    """ä»»åŠ¡å®Œæˆåçš„å¤„ç†"""
    print(f"[DEBUG] Task postrun: {task.name}, task_id={task_id}, state={state}", flush=True)
    logger.info(f"Task completed: {task.name}", task_id=task_id, state=state)

    # æ›´æ–°åŒæ­¥æœåŠ¡ç»Ÿè®¡ï¼ˆä»…é’ˆå¯¹æ³¨å†Œçš„æœåŠ¡ä»»åŠ¡ï¼‰
    print(f"[DEBUG] About to call _update_service_stats for {task.name}", flush=True)
    try:
        _update_service_stats(task.name, success=True, task_id=task_id)
        print(f"[DEBUG] _update_service_stats call completed", flush=True)
    except Exception as e:
        print(f"[DEBUG] âŒ Exception in _update_service_stats: {e}", flush=True)
        logger.error(f"Exception in _update_service_stats: {e}", exc_info=True)


@signals.task_failure.connect
def task_failure_handler(sender=None, task_id=None, exception=None, traceback=None, einfo=None, **kwds):
    """ä»»åŠ¡å¤±è´¥çš„å¤„ç†"""
    logger.error(f"Task failed: {sender.name}", task_id=task_id, exception=str(exception))

    # æ›´æ–°åŒæ­¥æœåŠ¡ç»Ÿè®¡ï¼ˆä»…é’ˆå¯¹æ³¨å†Œçš„æœåŠ¡ä»»åŠ¡ï¼‰
    _update_service_stats(sender.name, success=False, task_id=task_id, error_message=str(exception))


@signals.task_retry.connect
def task_retry_handler(sender=None, task_id=None, reason=None, einfo=None, **kwds):
    """ä»»åŠ¡é‡è¯•çš„å¤„ç†"""
    logger.warning(f"Task retrying: {sender.name}", task_id=task_id, reason=str(reason))


# ====================================================================================
# å…³é”®ï¼šåœ¨æ¨¡å—åŠ è½½æ—¶åˆå§‹åŒ–æ’ä»¶ï¼ˆç¡®ä¿ Celery Beat å’Œ Worker éƒ½èƒ½åŠ è½½æ’ä»¶ä»»åŠ¡ï¼‰
# ====================================================================================
def _initialize_plugins_for_celery():
    """
    Celery æ¨¡å—åŠ è½½æ—¶åˆå§‹åŒ–æ’ä»¶å¹¶æ³¨å†Œå®šæ—¶ä»»åŠ¡

    è¿™æ˜¯å…³é”®ï¼Celery Beat ä½œä¸ºç‹¬ç«‹è¿›ç¨‹å¯åŠ¨ï¼Œä¸ä¼šè§¦å‘ FastAPI çš„ startup äº‹ä»¶ï¼Œ
    æ‰€ä»¥å¿…é¡»åœ¨æ¨¡å—åŠ è½½æ—¶æ˜¾å¼åˆå§‹åŒ–æ’ä»¶ï¼Œå¦åˆ™æ’ä»¶æ³¨å†Œçš„å®šæ—¶ä»»åŠ¡ä¸ä¼šè¢«è°ƒåº¦ã€‚

    æ”¾åœ¨æ¨¡å—çº§åˆ«æ‰§è¡Œï¼Œç¡®ä¿æ— è®ºæ˜¯ Beat è¿˜æ˜¯ Workerï¼Œåªè¦å¯¼å…¥äº† celery_app å°±ä¼šåˆå§‹åŒ–ã€‚
    """
    try:
        import asyncio
        from ef_core.plugin_host import get_plugin_host
        from ef_core.tasks.registry import TaskRegistry
        from ef_core.event_bus import EventBus

        logger.info("ğŸ”§ Initializing plugins for Celery...")

        # åœ¨åŒä¸€ä¸ªäº‹ä»¶å¾ªç¯ä¸­å®Œæˆæ‰€æœ‰å¼‚æ­¥åˆå§‹åŒ–
        async def async_init():
            # åˆ›å»ºä»»åŠ¡æ³¨å†Œè¡¨å’Œäº‹ä»¶æ€»çº¿
            task_registry = TaskRegistry()
            event_bus = EventBus()

            try:
                # åˆå§‹åŒ–äº‹ä»¶æ€»çº¿
                await event_bus.initialize()

                # è·å–æ’ä»¶å®¿ä¸»å¹¶æ³¨å…¥ä¾èµ–
                plugin_host = get_plugin_host()
                plugin_host.task_registry = task_registry
                plugin_host.event_bus = event_bus

                # åˆå§‹åŒ–æ‰€æœ‰æ’ä»¶ï¼ˆä¼šè°ƒç”¨æ¯ä¸ªæ’ä»¶çš„ setup() å¹¶æ³¨å†Œå®šæ—¶ä»»åŠ¡ï¼‰
                await plugin_host.initialize()

                return task_registry
            finally:
                # å…³é”®ä¿®å¤ï¼šåœ¨äº‹ä»¶å¾ªç¯ç»“æŸå‰æ­£ç¡®å…³é—­ EventBusï¼Œé¿å… "Event loop is closed" é”™è¯¯
                await event_bus.shutdown()

        # åœ¨å•ä¸ªäº‹ä»¶å¾ªç¯ä¸­æ‰§è¡Œæ‰€æœ‰å¼‚æ­¥æ“ä½œ
        task_registry = asyncio.run(async_init())

        logger.info(f"âœ… Celery plugin initialization completed, registered {len(task_registry.registered_tasks)} tasks")

        # è¾“å‡ºæ³¨å†Œçš„ä»»åŠ¡åˆ—è¡¨ï¼ˆä¾¿äºè°ƒè¯•ï¼‰
        for task_name in task_registry.registered_tasks.keys():
            logger.info(f"  ğŸ“‹ Registered task: {task_name}")

    except Exception as e:
        logger.error(f"âŒ Failed to initialize plugins for Celery: {e}", exc_info=True)
        import traceback
        traceback.print_exc()

# ç«‹å³æ‰§è¡Œæ’ä»¶åˆå§‹åŒ–
_initialize_plugins_for_celery()

# ç¡®è®¤ä¿¡å·å¤„ç†å™¨å·²æ³¨å†Œ
print(f"[DEBUG] Celery signal handlers registered: task_postrun={len(signals.task_postrun.receivers)}, task_failure={len(signals.task_failure.receivers)}", flush=True)
logger.info(f"Celery signal handlers registered: task_postrun={len(signals.task_postrun.receivers)}, task_failure={len(signals.task_failure.receivers)}")


@signals.worker_ready.connect
def cleanup_stale_tasks_on_startup(**kwargs):
    """
    Worker å¯åŠ¨æ—¶æ¸…ç†æ‰€æœ‰åƒµæ­»çš„ä»»åŠ¡è¿›åº¦è®°å½•

    å½“ Celery worker é‡å¯æ—¶ï¼ŒRedis ä¸­å¯èƒ½è¿˜ä¿ç•™ç€æ—§çš„ä»»åŠ¡è¿›åº¦çŠ¶æ€ã€‚
    è¿™äº›çŠ¶æ€ä¼šå¯¼è‡´å‰ç«¯è®¤ä¸ºä»»åŠ¡è¿˜åœ¨è¿è¡Œï¼Œä½†å®é™…ä¸Š worker å·²ç»é‡å¯ï¼Œä»»åŠ¡å·²ç»ä¸¢å¤±ã€‚
    """
    try:
        import redis
        import json
        from celery.result import AsyncResult

        redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
        cleaned_count = 0
        zombie_count = 0

        for key in redis_client.keys("celery-task-progress:*"):
            try:
                data = redis_client.get(key)
                if not data:
                    continue

                progress = json.loads(data)
                task_id = key.replace("celery-task-progress:", "")

                # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
                if progress.get('status') in ['starting', 'syncing']:
                    # æ£€æŸ¥ Celery ä¸­ä»»åŠ¡æ˜¯å¦çœŸçš„åœ¨è¿è¡Œ
                    result = AsyncResult(task_id, app=celery_app)

                    # PENDING è¡¨ç¤ºä»»åŠ¡ä¸å­˜åœ¨æˆ–æœªå¼€å§‹ï¼ŒFAILURE/SUCCESS/REVOKED è¡¨ç¤ºä»»åŠ¡å·²ç»“æŸ
                    if result.state in ['PENDING', 'FAILURE', 'SUCCESS', 'REVOKED']:
                        redis_client.delete(key)
                        zombie_count += 1
                        logger.warning(f"Cleaned zombie task {task_id} with Celery state {result.state}")
                    else:
                        logger.info(f"Task {task_id} is still running with state {result.state}")
                elif progress.get('status') in ['completed', 'failed']:
                    # å·²å®Œæˆçš„ä»»åŠ¡ï¼Œæ¸…ç†æ‰
                    redis_client.delete(key)
                    cleaned_count += 1

            except Exception as e:
                logger.error(f"Error cleaning task progress {key}: {e}", exc_info=True)

        if zombie_count > 0 or cleaned_count > 0:
            logger.info(
                f"Startup cleanup completed: {zombie_count} zombie tasks, {cleaned_count} completed tasks removed"
            )
    except Exception as e:
        logger.error(f"Failed to cleanup stale tasks on startup: {e}", exc_info=True)


# ============================================================================
# å¯¼å…¥æ ¸å¿ƒä»»åŠ¡æ¨¡å—ä»¥æ³¨å†Œæ ¸å¿ƒç³»ç»Ÿä»»åŠ¡
# ============================================================================
# å¯¼å…¥ core_tasks æ¨¡å—ä¼šè‡ªåŠ¨æ‰§è¡Œå…¶ä¸­çš„ register_core_tasks() å‡½æ•°
# è¿™ä¼šå°†ä»¥ä¸‹æ ¸å¿ƒç³»ç»Ÿä»»åŠ¡æ·»åŠ åˆ° Celery Beat è°ƒåº¦ä¸­ï¼š
# - ef.core.system_health_check (æ¯5åˆ†é’Ÿ)
# - ef.core.cleanup_expired_data (æ¯å¤©å‡Œæ™¨3ç‚¹)
# - ef.core.metrics_collection (æ¯10åˆ†é’Ÿ)
# - ef.core.event_bus_maintenance (æ¯å¤©å‡Œæ™¨1:30)
from . import core_tasks  # noqa: F401