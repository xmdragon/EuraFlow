"""
Celery åº”ç”¨é…ç½®
"""
from celery import Celery, signals
from celery.schedules import crontab
from ef_core.config import get_settings
from ef_core.utils.logger import get_logger

logger = get_logger(__name__)

# è·å–é…ç½®
settings = get_settings()

# åˆ›å»º Celery åº”ç”¨
celery_app = Celery(
    "euraflow",
    broker=settings.celery_broker_url,
    backend=settings.celery_result_backend,
    include=[
        "ef_core.tasks.core_tasks",
        "plugins.ef.channels.ozon.tasks",  # Ozonæ’ä»¶ä»»åŠ¡
    ]
)

# Celery é…ç½®
celery_app.conf.update(
    # ä»»åŠ¡åºåˆ—åŒ–
    task_serializer=settings.celery_task_serializer,
    result_serializer=settings.celery_result_serializer,
    accept_content=settings.celery_accept_content,
    
    # æ—¶åŒºè®¾ç½®
    timezone=settings.celery_timezone,
    enable_utc=True,
    
    # ä»»åŠ¡è·¯ç”±
    task_routes={
        "ef.core.*": {"queue": "ef_core"},
        "ef.*.pull_orders": {"queue": "ef_pull"},
        "ef.*.push_*": {"queue": "ef_push"},
    },
    
    # ä»»åŠ¡ç»“æœé…ç½®
    result_expires=3600,  # 1å°æ—¶åè¿‡æœŸ
    task_ignore_result=False,
    
    # Worker é…ç½®
    worker_prefetch_multiplier=1,  # å…¬å¹³è°ƒåº¦
    task_acks_late=True,  # ä»»åŠ¡å®Œæˆåç¡®è®¤
    worker_max_tasks_per_child=1000,  # æ¯ä¸ªworkerå¤„ç†1000ä¸ªä»»åŠ¡åé‡å¯
    
    # é‡è¯•é…ç½®
    task_default_retry_delay=60,  # 60ç§’åé‡è¯•
    task_max_retries=5,  # æœ€å¤šé‡è¯•5æ¬¡

    # ä»»åŠ¡è¶…æ—¶é…ç½®ï¼ˆé˜²æ­¢åƒµå°¸ä»»åŠ¡ï¼‰
    task_soft_time_limit=300,  # 5åˆ†é’Ÿè½¯è¶…æ—¶ï¼ˆè§¦å‘ SoftTimeLimitExceeded å¼‚å¸¸ï¼‰
    task_time_limit=360,  # 6åˆ†é’Ÿç¡¬è¶…æ—¶ï¼ˆå¼ºåˆ¶ç»ˆæ­¢ä»»åŠ¡ï¼‰

    # é€Ÿç‡é™åˆ¶
    task_annotations={
        "*": {"rate_limit": "100/m"},  # å…¨å±€é™åˆ¶
        # é•¿æ—¶é—´ä»»åŠ¡å•ç‹¬é…ç½®è¶…æ—¶
        "ef.ozon.orders.pull": {"soft_time_limit": 600, "time_limit": 660},  # 10åˆ†é’Ÿ
        "ef.ozon.inventory.sync": {"soft_time_limit": 600, "time_limit": 660},  # 10åˆ†é’Ÿ
        "ef.ozon.promotions.sync": {"soft_time_limit": 600, "time_limit": 660},  # 10åˆ†é’Ÿ
        "ef.ozon.batch_update_stocks": {"soft_time_limit": 900, "time_limit": 960},  # 15åˆ†é’Ÿ
    },
    
    # ç›‘æ§é…ç½®
    worker_send_task_events=True,
    task_send_sent_event=True,
    
    # ä»»åŠ¡å‹ç¼©
    task_compression="gzip",
    result_compression="gzip",
)

# å®šæœŸä»»åŠ¡é…ç½®ï¼ˆBeat Scheduleï¼‰
celery_app.conf.beat_schedule = {
    # ç³»ç»Ÿå¥åº·æ£€æŸ¥
    "system-health-check": {
        "task": "ef.core.health_check",
        "schedule": crontab(minute="*/5"),  # æ¯5åˆ†é’Ÿ
        "options": {"queue": "ef_core"}
    },

    # æ¸…ç†è¿‡æœŸä»»åŠ¡ç»“æœ
    "cleanup-expired-results": {
        "task": "ef.core.cleanup_results",
        "schedule": crontab(hour="2", minute="0"),  # æ¯å¤©å‡Œæ™¨2ç‚¹
        "options": {"queue": "ef_core"}
    },

    # æ³¨æ„ï¼šOZON ç±»ç›®åŒæ­¥å’Œç‰¹å¾åŒæ­¥ä»»åŠ¡å·²ç§»é™¤ï¼ˆæœªå®ç°å¯¹åº”çš„ä»»åŠ¡å‡½æ•°ï¼‰
    # å¦‚éœ€å¯ç”¨ï¼Œéœ€åœ¨ OZON æ’ä»¶ä¸­æ³¨å†Œå¯¹åº”çš„ Celery ä»»åŠ¡

    # æ’ä»¶ä»»åŠ¡å°†é€šè¿‡ TaskRegistry åŠ¨æ€æ³¨å†Œ
}


@celery_app.task(bind=True, name="ef.core.health_check")
def health_check(self):
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥ä»»åŠ¡"""
    try:
        logger.info("Performing system health check")
        
        # TODO: æ£€æŸ¥æ•°æ®åº“è¿æ¥
        # TODO: æ£€æŸ¥ Redis è¿æ¥
        # TODO: æ£€æŸ¥å¤–éƒ¨ API è¿æ¥
        
        logger.info("System health check completed", result="healthy")
        return {"status": "healthy", "timestamp": "2025-01-01T00:00:00Z"}
    
    except Exception as e:
        logger.error("System health check failed", exc_info=True)
        raise self.retry(countdown=60, max_retries=3)


@celery_app.task(bind=True, name="ef.core.cleanup_results")
def cleanup_results(self, older_than_days=1):
    """
    æ¸…ç†è¿‡æœŸçš„ä»»åŠ¡ç»“æœ

    Args:
        older_than_days: æ¸…ç†å¤šå°‘å¤©å‰çš„ç»“æœï¼ˆé»˜è®¤1å¤©ï¼‰

    Returns:
        æ¸…ç†ç»Ÿè®¡ä¿¡æ¯
    """
    import time
    from datetime import datetime, UTC

    try:
        logger.info(f"Starting cleanup of task results older than {older_than_days} days")

        # è·å– Redis è¿æ¥
        redis_client = celery_app.backend.client

        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "started_at": datetime.now(UTC).isoformat(),
            "scanned": 0,
            "cleaned": 0,
            "errors": 0,
            "older_than_days": older_than_days
        }

        # è®¡ç®—è¿‡æœŸæ—¶é—´æˆ³ï¼ˆç§’ï¼‰
        cutoff_timestamp = time.time() - (older_than_days * 24 * 3600)

        # æ‰«ææ‰€æœ‰ Celery ä»»åŠ¡ç»“æœé”®
        # Celery é»˜è®¤ä½¿ç”¨ "celery-task-meta-{task_id}" ä½œä¸ºé”®å
        cursor = 0
        batch_size = 1000

        while True:
            # ä½¿ç”¨ SCAN å‘½ä»¤æ‰¹é‡æ‰«æé”®ï¼ˆé¿å…é˜»å¡ Redisï¼‰
            cursor, keys = redis_client.scan(
                cursor=cursor,
                match="celery-task-meta-*",
                count=batch_size
            )

            stats["scanned"] += len(keys)

            # æ£€æŸ¥æ¯ä¸ªé”®
            for key in keys:
                try:
                    # è·å–é”®çš„ TTLï¼ˆå‰©ä½™ç”Ÿå­˜æ—¶é—´ï¼‰
                    ttl = redis_client.ttl(key)

                    # TTL = -1 è¡¨ç¤ºæ°¸ä¸è¿‡æœŸï¼ŒTTL = -2 è¡¨ç¤ºé”®ä¸å­˜åœ¨
                    if ttl == -1:
                        # å¯¹äºæ°¸ä¸è¿‡æœŸçš„é”®ï¼Œæ£€æŸ¥åˆ›å»ºæ—¶é—´
                        # å°è¯•è·å–ä»»åŠ¡ç»“æœçš„æ—¶é—´æˆ³
                        result = redis_client.get(key)
                        if result:
                            # Celery ç»“æœåŒ…å« date_done å­—æ®µ
                            import json
                            try:
                                result_data = json.loads(result)
                                date_done = result_data.get('date_done')

                                if date_done:
                                    # è§£ææ—¶é—´æˆ³
                                    from dateutil import parser
                                    done_time = parser.parse(date_done)
                                    done_timestamp = done_time.timestamp()

                                    # å¦‚æœè¶…è¿‡é˜ˆå€¼ï¼Œåˆ é™¤
                                    if done_timestamp < cutoff_timestamp:
                                        redis_client.delete(key)
                                        stats["cleaned"] += 1
                            except (json.JSONDecodeError, ValueError):
                                # æ— æ³•è§£æçš„ç»“æœï¼Œè·³è¿‡
                                pass

                    elif ttl == -2:
                        # é”®å·²ç»ä¸å­˜åœ¨ï¼Œè·³è¿‡
                        pass

                except Exception as e:
                    logger.warning(f"Error processing key {key}: {e}")
                    stats["errors"] += 1

            # å¦‚æœæ¸¸æ ‡å›åˆ°0ï¼Œè¡¨ç¤ºæ‰«æå®Œæˆ
            if cursor == 0:
                break

        # é¢å¤–æ¸…ç†ï¼šåˆ é™¤å·²è¿‡æœŸä½†æœªè¢«è‡ªåŠ¨æ¸…ç†çš„é”®
        # Celery æœ‰æ—¶ä¼šç•™ä¸‹ä¸€äº›è¿‡æœŸé”®
        cursor = 0
        while True:
            cursor, keys = redis_client.scan(
                cursor=cursor,
                match="celery-task-meta-*",
                count=batch_size
            )

            for key in keys:
                try:
                    ttl = redis_client.ttl(key)
                    # å¦‚æœ TTL <= 0ï¼ˆå·²è¿‡æœŸï¼‰ï¼Œç›´æ¥åˆ é™¤
                    if ttl == -2 or (ttl > 0 and ttl < 0):
                        redis_client.delete(key)
                        stats["cleaned"] += 1
                except Exception as e:
                    stats["errors"] += 1

            if cursor == 0:
                break

        # è®°å½•å®Œæˆæ—¶é—´
        stats["completed_at"] = datetime.now(UTC).isoformat()

        logger.info(
            f"Task results cleanup completed",
            scanned=stats["scanned"],
            cleaned=stats["cleaned"],
            errors=stats["errors"]
        )

        return stats

    except Exception as e:
        logger.error("Task results cleanup failed", exc_info=True)
        return {
            "error": str(e),
            "scanned": 0,
            "cleaned": 0,
            "errors": 1
        }


# å­˜å‚¨ä»»åŠ¡ log_id çš„ä¸Šä¸‹æ–‡ï¼ˆä»»åŠ¡å¼€å§‹æ—¶åˆ›å»ºï¼Œç»“æŸæ—¶æ›´æ–°ï¼‰
_task_log_ids = {}

# å­˜å‚¨ä»»åŠ¡å¼€å§‹æ—¶é—´
_task_start_times = {}


def _record_task_start(task_name: str, task_id: str):
    """
    è®°å½•ä»»åŠ¡å¼€å§‹ï¼ˆä½¿ç”¨åŒæ­¥æ•°æ®åº“æ“ä½œï¼‰

    Args:
        task_name: Celery ä»»åŠ¡å
        task_id: ä»»åŠ¡ID

    Returns:
        log_idï¼Œç”¨äºåç»­æ›´æ–°
    """
    import time
    _task_start_times[task_id] = time.time()

    try:
        from datetime import datetime, timezone
        from sqlalchemy import create_engine, select
        from sqlalchemy.orm import sessionmaker
        from ef_core.config import get_settings
        from plugins.ef.system.sync_service.models.sync_service import SyncService
        from plugins.ef.system.sync_service.models.sync_service_log import SyncServiceLog

        settings = get_settings()
        sync_db_url = settings.database_url.replace('+asyncpg', '')
        engine = create_engine(sync_db_url, pool_pre_ping=True, pool_recycle=3600)
        SessionLocal = sessionmaker(bind=engine)

        with SessionLocal() as db:
            # é€šè¿‡ celery_task_name æŸ¥æ‰¾æœåŠ¡
            stmt = select(SyncService).where(SyncService.celery_task_name == task_name)
            service = db.execute(stmt).scalar_one_or_none()

            if not service:
                logger.debug(f"Service not found for task: {task_name}")
                return None

            # æ›´æ–°æœåŠ¡çŠ¶æ€
            service.last_run_at = datetime.now(timezone.utc)
            service.last_run_status = "running"

            # åˆ›å»ºæ—¥å¿—è®°å½•
            log = SyncServiceLog(
                service_key=service.service_key,
                run_id=task_id,
                started_at=datetime.now(timezone.utc),
                status="running",
            )
            db.add(log)
            db.commit()
            db.refresh(log)

            logger.debug(f"Task started: {task_name}, log_id={log.id}")
            return log.id

    except Exception as e:
        logger.error(f"Failed to record task start for {task_name}: {e}", exc_info=True)
        return None


def _record_task_end(task_name: str, task_id: str, success: bool, error_message: str = None):
    """
    è®°å½•ä»»åŠ¡ç»“æŸï¼ˆä½¿ç”¨åŒæ­¥æ•°æ®åº“æ“ä½œï¼‰

    Args:
        task_name: Celery ä»»åŠ¡å
        task_id: ä»»åŠ¡ID
        success: æ˜¯å¦æˆåŠŸ
        error_message: é”™è¯¯ä¿¡æ¯ï¼ˆå¤±è´¥æ—¶ï¼‰
    """
    import time

    # è®¡ç®—æ‰§è¡Œæ—¶é—´
    start_time = _task_start_times.pop(task_id, None)
    execution_time_ms = int((time.time() - start_time) * 1000) if start_time else 0

    # è·å– log_id
    log_id = _task_log_ids.pop(task_id, None)

    try:
        from datetime import datetime, timezone
        from sqlalchemy import create_engine, select
        from sqlalchemy.orm import sessionmaker
        from ef_core.config import get_settings
        from plugins.ef.system.sync_service.models.sync_service import SyncService
        from plugins.ef.system.sync_service.models.sync_service_log import SyncServiceLog

        settings = get_settings()
        sync_db_url = settings.database_url.replace('+asyncpg', '')
        engine = create_engine(sync_db_url, pool_pre_ping=True, pool_recycle=3600)
        SessionLocal = sessionmaker(bind=engine)

        with SessionLocal() as db:
            # æ›´æ–°æ—¥å¿—è®°å½•
            if log_id:
                stmt = select(SyncServiceLog).where(SyncServiceLog.id == log_id)
                log = db.execute(stmt).scalar_one_or_none()

                if log:
                    log.finished_at = datetime.now(timezone.utc)
                    log.status = "success" if success else "failed"
                    log.execution_time_ms = execution_time_ms
                    if error_message:
                        log.error_message = error_message[:2000]  # æˆªæ–­è¿‡é•¿çš„é”™è¯¯ä¿¡æ¯

            # æ›´æ–°æœåŠ¡ç»Ÿè®¡
            stmt = select(SyncService).where(SyncService.celery_task_name == task_name)
            service = db.execute(stmt).scalar_one_or_none()

            if service:
                service.run_count = (service.run_count or 0) + 1
                if success:
                    service.success_count = (service.success_count or 0) + 1
                    service.last_run_status = "success"
                    service.last_run_message = f"æ‰§è¡ŒæˆåŠŸï¼Œè€—æ—¶ {execution_time_ms}ms"
                else:
                    service.error_count = (service.error_count or 0) + 1
                    service.last_run_status = "failed"
                    service.last_run_message = error_message[:500] if error_message else "æ‰§è¡Œå¤±è´¥"

            db.commit()
            logger.debug(f"Task ended: {task_name}, success={success}, time={execution_time_ms}ms")

    except Exception as e:
        logger.error(f"Failed to record task end for {task_name}: {e}", exc_info=True)


def _sync_registry_to_database(registered_tasks: dict):
    """
    åŒæ­¥ TaskRegistry åˆ°æ•°æ®åº“

    ç­–ç•¥ï¼š
    1. éå† registry ä¸­çš„æ‰€æœ‰ä»»åŠ¡
    2. å¦‚æœæ•°æ®åº“ä¸­ä¸å­˜åœ¨è¯¥ celery_task_nameï¼Œåˆ™åˆ›å»º
    3. å¦‚æœå­˜åœ¨ä½†é…ç½®ä¸åŒï¼Œæ›´æ–°ï¼ˆä»…æ›´æ–° source=code çš„è®°å½•ï¼‰
    4. æ ‡è®°æ•°æ®åº“ä¸­å­˜åœ¨ä½† registry ä¸­ä¸å­˜åœ¨çš„ä»»åŠ¡ä¸º is_deleted=True
    """
    try:
        from sqlalchemy import create_engine, select
        from sqlalchemy.orm import sessionmaker
        from ef_core.config import get_settings
        from plugins.ef.system.sync_service.models.sync_service import SyncService

        settings = get_settings()
        sync_db_url = settings.database_url.replace('+asyncpg', '')
        engine = create_engine(sync_db_url, pool_pre_ping=True, pool_recycle=3600)
        SessionLocal = sessionmaker(bind=engine)

        created = 0
        updated = 0
        deleted = 0

        with SessionLocal() as db:
            # è·å–æ‰€æœ‰ç°æœ‰æœåŠ¡ï¼ˆæŒ‰ celery_task_nameï¼‰
            stmt = select(SyncService)
            existing_services = {s.celery_task_name: s for s in db.execute(stmt).scalars().all() if s.celery_task_name}

            # è·å–æ‰€æœ‰ç°æœ‰ service_key
            all_services = {s.service_key: s for s in db.execute(select(SyncService)).scalars().all()}

            registered_task_names = set(registered_tasks.keys())

            # åŒæ­¥ä»»åŠ¡
            for task_name, task_info in registered_tasks.items():
                cron = task_info.get("cron", "")
                plugin = task_info.get("plugin", "")
                display_name = task_info.get("display_name") or task_name
                description = task_info.get("description") or ""

                # ç”Ÿæˆ service_keyï¼ˆä»ä»»åŠ¡åè½¬æ¢ï¼‰
                service_key = task_name.replace(".", "_")

                if task_name in existing_services:
                    # å·²å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                    service = existing_services[task_name]
                    if service.source == "code":
                        # åªæ›´æ–°ä»£ç æ¥æºçš„é…ç½®
                        need_update = False
                        if service.schedule_config != cron:
                            service.schedule_config = cron
                            need_update = True
                        if service.service_name != display_name:
                            service.service_name = display_name
                            need_update = True
                        if service.service_description != description:
                            service.service_description = description
                            need_update = True
                        if service.is_deleted:
                            service.is_deleted = False
                            need_update = True

                        if need_update:
                            updated += 1
                            logger.debug(f"Updated sync service: {task_name}")
                elif service_key in all_services:
                    # service_key å­˜åœ¨ä½†æ²¡æœ‰ celery_task_nameï¼Œæ›´æ–°å®ƒ
                    service = all_services[service_key]
                    service.celery_task_name = task_name
                    service.plugin_name = plugin
                    service.source = "code"
                    service.is_deleted = False
                    if service.schedule_config != cron:
                        service.schedule_config = cron
                    updated += 1
                    logger.debug(f"Linked existing service {service_key} to {task_name}")
                else:
                    # åˆ›å»ºæ–°æœåŠ¡
                    new_service = SyncService(
                        service_key=service_key,
                        service_name=display_name,
                        service_description=description,
                        service_type="cron",
                        schedule_config=cron,
                        is_enabled=True,
                        celery_task_name=task_name,
                        plugin_name=plugin,
                        source="code",
                        is_deleted=False,
                    )
                    db.add(new_service)
                    created += 1
                    logger.debug(f"Created sync service: {task_name}")

            # æ ‡è®°å·²åˆ é™¤çš„ä»»åŠ¡
            for task_name, service in existing_services.items():
                if task_name not in registered_task_names and service.source == "code" and not service.is_deleted:
                    service.is_deleted = True
                    deleted += 1
                    logger.debug(f"Marked sync service as deleted: {task_name}")

            db.commit()

        logger.info(f"ğŸ“Š Synced registry to database: created={created}, updated={updated}, deleted={deleted}")

    except Exception as e:
        logger.error(f"Failed to sync registry to database: {e}", exc_info=True)


# Celery ä¿¡å·å¤„ç†å™¨ï¼ˆæ³¨æ„ï¼šå¿…é¡»åœ¨æ¨¡å—çº§åˆ«å®šä¹‰ï¼ŒWorker å¯åŠ¨æ—¶ä¼šè‡ªåŠ¨æ³¨å†Œï¼‰
@signals.task_prerun.connect
def task_prerun_handler(sender=None, task_id=None, task=None, args=None, kwargs=None, **kwds):
    """ä»»åŠ¡å¼€å§‹å‰çš„å¤„ç†"""
    logger.debug(f"Task prerun: {task.name}, task_id={task_id}")
    logger.info(f"Task starting: {task.name}", task_id=task_id)

    # è®°å½•ä»»åŠ¡å¼€å§‹åˆ° sync_service_logs
    try:
        log_id = _record_task_start(task.name, task_id)
        if log_id:
            _task_log_ids[task_id] = log_id
    except Exception as e:
        logger.error(f"Failed to record task start: {e}", exc_info=True)


@signals.task_postrun.connect
def task_postrun_handler(sender=None, task_id=None, task=None, args=None, kwargs=None, retval=None, state=None, **kwds):
    """ä»»åŠ¡å®Œæˆåçš„å¤„ç†"""
    logger.debug(f"Task postrun: {task.name}, task_id={task_id}, state={state}")
    logger.info(f"Task completed: {task.name}", task_id=task_id, state=state)

    # è®°å½•ä»»åŠ¡ç»“æŸåˆ° sync_service_logs
    try:
        _record_task_end(task.name, task_id, success=True)
    except Exception as e:
        logger.error(f"Failed to record task end: {e}", exc_info=True)


@signals.task_failure.connect
def task_failure_handler(sender=None, task_id=None, exception=None, traceback=None, einfo=None, **kwds):
    """ä»»åŠ¡å¤±è´¥çš„å¤„ç†"""
    logger.error(f"Task failed: {sender.name}", task_id=task_id, exception=str(exception))

    # è®°å½•ä»»åŠ¡å¤±è´¥åˆ° sync_service_logs
    try:
        _record_task_end(sender.name, task_id, success=False, error_message=str(exception))
    except Exception as e:
        logger.error(f"Failed to record task failure: {e}", exc_info=True)


@signals.task_retry.connect
def task_retry_handler(sender=None, task_id=None, reason=None, einfo=None, **kwds):
    """ä»»åŠ¡é‡è¯•çš„å¤„ç†"""
    logger.warning(f"Task retrying: {sender.name}", task_id=task_id, reason=str(reason))


# ====================================================================================
# å…³é”®ï¼šåœ¨æ¨¡å—åŠ è½½æ—¶åˆå§‹åŒ–æ’ä»¶ï¼ˆç¡®ä¿ Celery Beat å’Œ Worker éƒ½èƒ½åŠ è½½æ’ä»¶ä»»åŠ¡ï¼‰
# ====================================================================================
def _initialize_plugins_for_celery():
    """
    Celery æ¨¡å—åŠ è½½æ—¶åˆå§‹åŒ–æ’ä»¶å¹¶æ³¨å†Œå®šæ—¶ä»»åŠ¡

    è¿™æ˜¯å…³é”®ï¼Celery Beat ä½œä¸ºç‹¬ç«‹è¿›ç¨‹å¯åŠ¨ï¼Œä¸ä¼šè§¦å‘ FastAPI çš„ startup äº‹ä»¶ï¼Œ
    æ‰€ä»¥å¿…é¡»åœ¨æ¨¡å—åŠ è½½æ—¶æ˜¾å¼åˆå§‹åŒ–æ’ä»¶ï¼Œå¦åˆ™æ’ä»¶æ³¨å†Œçš„å®šæ—¶ä»»åŠ¡ä¸ä¼šè¢«è°ƒåº¦ã€‚

    æ”¾åœ¨æ¨¡å—çº§åˆ«æ‰§è¡Œï¼Œç¡®ä¿æ— è®ºæ˜¯ Beat è¿˜æ˜¯ Workerï¼Œåªè¦å¯¼å…¥äº† celery_app å°±ä¼šåˆå§‹åŒ–ã€‚
    """
    try:
        import asyncio
        from ef_core.plugin_host import get_plugin_host
        from ef_core.tasks.registry import TaskRegistry
        from ef_core.event_bus import EventBus

        logger.info("ğŸ”§ Initializing plugins for Celery...")

        # åœ¨åŒä¸€ä¸ªäº‹ä»¶å¾ªç¯ä¸­å®Œæˆæ‰€æœ‰å¼‚æ­¥åˆå§‹åŒ–
        async def async_init():
            # åˆ›å»ºä»»åŠ¡æ³¨å†Œè¡¨å’Œäº‹ä»¶æ€»çº¿
            task_registry = TaskRegistry()
            event_bus = EventBus()

            try:
                # åˆå§‹åŒ–äº‹ä»¶æ€»çº¿
                await event_bus.initialize()

                # è·å–æ’ä»¶å®¿ä¸»å¹¶æ³¨å…¥ä¾èµ–
                plugin_host = get_plugin_host()
                plugin_host.task_registry = task_registry
                plugin_host.event_bus = event_bus

                # åˆå§‹åŒ–æ‰€æœ‰æ’ä»¶ï¼ˆä¼šè°ƒç”¨æ¯ä¸ªæ’ä»¶çš„ setup() å¹¶æ³¨å†Œå®šæ—¶ä»»åŠ¡ï¼‰
                await plugin_host.initialize()

                return task_registry
            finally:
                # å…³é”®ä¿®å¤ï¼šåœ¨äº‹ä»¶å¾ªç¯ç»“æŸå‰æ­£ç¡®å…³é—­ EventBusï¼Œé¿å… "Event loop is closed" é”™è¯¯
                await event_bus.shutdown()

        # åœ¨å•ä¸ªäº‹ä»¶å¾ªç¯ä¸­æ‰§è¡Œæ‰€æœ‰å¼‚æ­¥æ“ä½œ
        # å…ˆæ£€æµ‹æ˜¯å¦å·²æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œé¿å…åˆ›å»ºä¸ä¼šè¢« await çš„åç¨‹
        try:
            asyncio.get_running_loop()
            has_running_loop = True
        except RuntimeError:
            has_running_loop = False

        if has_running_loop:
            # Celery worker ç¯å¢ƒä¸­å·²æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯
            # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯æ¥æ‰§è¡Œåˆå§‹åŒ–
            logger.warning("Detected running event loop, creating new event loop for plugin initialization")
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                task_registry = loop.run_until_complete(async_init())
            finally:
                loop.close()
                # é‡è¦ï¼šæ¸…ç†äº‹ä»¶å¾ªç¯å¼•ç”¨ï¼Œé¿å…åç»­å†²çª
                asyncio.set_event_loop(None)
        else:
            # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯
            # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯ä½†ä¸åœ¨ç»“æŸåæ¸…é™¤ï¼Œä»¥ä¾¿ ARQ ç­‰å…¶ä»–ç»„ä»¶å¯ä»¥ä½¿ç”¨
            # æ³¨æ„ï¼šasyncio.run() ä¼šåœ¨ç»“æŸåå…³é—­äº‹ä»¶å¾ªç¯ï¼Œå¯¼è‡´åç»­ get_event_loop() å¤±è´¥
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                task_registry = loop.run_until_complete(async_init())
            finally:
                # å…³é—­äº‹ä»¶å¾ªç¯ä½†åˆ›å»ºä¸€ä¸ªæ–°çš„ç©ºå¾ªç¯ä¾›åç»­ä½¿ç”¨
                loop.close()
                # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯ä¾›å…¶ä»–ç»„ä»¶ï¼ˆå¦‚ ARQï¼‰ä½¿ç”¨
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)

        logger.info(f"âœ… Celery plugin initialization completed, registered {len(task_registry.registered_tasks)} tasks")

        # è¾“å‡ºæ³¨å†Œçš„ä»»åŠ¡åˆ—è¡¨ï¼ˆä¾¿äºè°ƒè¯•ï¼‰
        for task_name in task_registry.registered_tasks.keys():
            logger.info(f"  ğŸ“‹ Registered task: {task_name}")

        # åŒæ­¥ä»»åŠ¡æ³¨å†Œè¡¨åˆ°æ•°æ®åº“
        _sync_registry_to_database(task_registry.registered_tasks)

    except Exception as e:
        logger.error(f"Failed to initialize plugins for Celery: {e}", exc_info=True)

# ç«‹å³æ‰§è¡Œæ’ä»¶åˆå§‹åŒ–
_initialize_plugins_for_celery()

# ç¡®è®¤ä¿¡å·å¤„ç†å™¨å·²æ³¨å†Œ
logger.info(f"Celery signal handlers registered: task_postrun={len(signals.task_postrun.receivers)}, task_failure={len(signals.task_failure.receivers)}")


@signals.worker_ready.connect
def cleanup_stale_tasks_on_startup(**kwargs):
    """
    Worker å¯åŠ¨æ—¶æ¸…ç†æ‰€æœ‰åƒµæ­»çš„ä»»åŠ¡è¿›åº¦è®°å½•

    å½“ Celery worker é‡å¯æ—¶ï¼ŒRedis ä¸­å¯èƒ½è¿˜ä¿ç•™ç€æ—§çš„ä»»åŠ¡è¿›åº¦çŠ¶æ€ã€‚
    è¿™äº›çŠ¶æ€ä¼šå¯¼è‡´å‰ç«¯è®¤ä¸ºä»»åŠ¡è¿˜åœ¨è¿è¡Œï¼Œä½†å®é™…ä¸Š worker å·²ç»é‡å¯ï¼Œä»»åŠ¡å·²ç»ä¸¢å¤±ã€‚
    """
    try:
        import redis
        import json
        from celery.result import AsyncResult

        redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
        cleaned_count = 0
        zombie_count = 0

        for key in redis_client.keys("celery-task-progress:*"):
            try:
                data = redis_client.get(key)
                if not data:
                    continue

                progress = json.loads(data)
                task_id = key.replace("celery-task-progress:", "")

                # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
                if progress.get('status') in ['starting', 'syncing']:
                    # æ£€æŸ¥ Celery ä¸­ä»»åŠ¡æ˜¯å¦çœŸçš„åœ¨è¿è¡Œ
                    result = AsyncResult(task_id, app=celery_app)

                    # PENDING è¡¨ç¤ºä»»åŠ¡ä¸å­˜åœ¨æˆ–æœªå¼€å§‹ï¼ŒFAILURE/SUCCESS/REVOKED è¡¨ç¤ºä»»åŠ¡å·²ç»“æŸ
                    if result.state in ['PENDING', 'FAILURE', 'SUCCESS', 'REVOKED']:
                        redis_client.delete(key)
                        zombie_count += 1
                        logger.warning(f"Cleaned zombie task {task_id} with Celery state {result.state}")
                    else:
                        logger.info(f"Task {task_id} is still running with state {result.state}")
                elif progress.get('status') in ['completed', 'failed']:
                    # å·²å®Œæˆçš„ä»»åŠ¡ï¼Œæ¸…ç†æ‰
                    redis_client.delete(key)
                    cleaned_count += 1

            except Exception as e:
                logger.error(f"Error cleaning task progress {key}: {e}", exc_info=True)

        if zombie_count > 0 or cleaned_count > 0:
            logger.info(
                f"Startup cleanup completed: {zombie_count} zombie tasks, {cleaned_count} completed tasks removed"
            )
    except Exception as e:
        logger.error(f"Failed to cleanup stale tasks on startup: {e}", exc_info=True)


# ============================================================================
# å¯¼å…¥æ ¸å¿ƒä»»åŠ¡æ¨¡å—ä»¥æ³¨å†Œæ ¸å¿ƒç³»ç»Ÿä»»åŠ¡
# ============================================================================
# å¯¼å…¥ core_tasks æ¨¡å—ä¼šè‡ªåŠ¨æ‰§è¡Œå…¶ä¸­çš„ register_core_tasks() å‡½æ•°
# è¿™ä¼šå°†ä»¥ä¸‹æ ¸å¿ƒç³»ç»Ÿä»»åŠ¡æ·»åŠ åˆ° Celery Beat è°ƒåº¦ä¸­ï¼š
# - ef.core.system_health_check (æ¯5åˆ†é’Ÿ)
# - ef.core.cleanup_expired_data (æ¯å¤©å‡Œæ™¨3ç‚¹)
# - ef.core.metrics_collection (æ¯10åˆ†é’Ÿ)
# - ef.core.event_bus_maintenance (æ¯å¤©å‡Œæ™¨1:30)
from . import core_tasks  # noqa: F401